batch_size: 64
data_params:
  bg_color:
    maxs:
    - - 1.0
      - 1.0
      - 1.0
    - - 0.2
      - 0.2
      - 0.2
    means:
    - - 0.9
      - 0.9
      - 0.9
    - - 0.1
      - 0.1
      - 0.1
    mins:
    - - 0.8
      - 0.8
      - 0.8
    - - 0.0
      - 0.0
      - 0.0
    s_n: 0.0
  color:
    maxs:
    - - 0.60875
      - 0.43475
      - 0.43475
    - - 0.43475
      - 0.43475
      - 0.60875
    means:
    - - 0.587
      - 0.41300000000000003
      - 0.41300000000000003
    - - 0.41300000000000003
      - 0.41300000000000003
      - 0.587
    mins:
    - - 0.5652499999999999
      - 0.39125000000000004
      - 0.39125000000000004
    - - 0.39125000000000004
      - 0.39125000000000004
      - 0.5652499999999999
    s_n: 0.0
  comp_dims:
    bg_color: null
    color: 2
    shape: null
    size: 2
    x: null
    y: null
  image_size: 32
  n_classes: 4
  noise_level: 0.001
  size:
    maxs:
    - 0.8
    - 0.4
    means:
    - 0.65
    - 0.25
    min: 0.1
    mins:
    - 0.5
    - 0.09999999999999998
    s_n: 0.0
  x:
    max: 0.5
    min: -0.5
    n: null
    s: 0.05
    s_n: 0.0
  y:
    max: 0.5
    min: -0.5
    n: null
    s: 0.05
    s_n: 0.0
dataset: images_1
experiment_directory: data/images_1/2x2_final2/mirun_manyckpt5/sep_col=0.174_sep_size=0.4/seed=100
fig_n_col: 2
model_params:
  beta_settings:
    gamma_max: 10.0
    gamma_min: -5.0
    noise_schedule: learned_linear
    type: logsnr
  data_noise: 0.001
  model_type: VDiff
  network_params:
    chs:
    - 64
    - 128
    - 256
    init_scale: 1.0
    mid_attn: true
    num_res_blocks: 2
    v_conditioning_dims:
    - 11
    v_conditioning_type: sep_mlp
  optimizer_params:
    lr: 0.001
    weight_decay: 0.01
  optimizer_type: AdamW
  p_cfg: null
  w_cfg: null
n_samples_test:
- 0
- 0
- 0
- 32
n_samples_test_gen:
- 0
- 0
- 0
- 32
n_samples_train:
- 2048
- 2048
- 2048
- 0
n_samples_train_gen:
- 32
- 32
- 32
- 0
num_steps: 20000
save_steps:
- 50
- 100
- 168
- 254
- 357
- 477
- 615
- 770
- 943
- 1134
- 1341
- 1567
- 1810
- 2070
- 2348
- 2643
- 2956
- 3286
- 3633
- 3999
- 4000
- 4080
- 4160
- 4241
- 4321
- 4402
- 4482
- 4562
- 4643
- 4723
- 4804
- 4884
- 4964
- 5045
- 5125
- 5206
- 5286
- 5366
- 5447
- 5527
- 5608
- 5688
- 5768
- 5849
- 5929
- 6010
- 6090
- 6170
- 6251
- 6331
- 6412
- 6492
- 6572
- 6653
- 6733
- 6814
- 6894
- 6974
- 7055
- 7135
- 7216
- 7296
- 7376
- 7457
- 7537
- 7618
- 7698
- 7778
- 7859
- 7939
- 8020
- 8100
- 8180
- 8261
- 8341
- 8422
- 8502
- 8582
- 8663
- 8743
- 8824
- 8904
- 8984
- 9065
- 9145
- 9226
- 9306
- 9386
- 9467
- 9547
- 9628
- 9708
- 9788
- 9869
- 9949
- 10030
- 10110
- 10190
- 10271
- 10351
- 10432
- 10512
- 10592
- 10673
- 10753
- 10834
- 10914
- 10994
- 11075
- 11155
- 11236
- 11316
- 11396
- 11477
- 11557
- 11638
- 11718
- 11798
- 11879
- 11959
- 12040
- 12120
- 12201
- 12281
- 12361
- 12442
- 12522
- 12603
- 12683
- 12763
- 12844
- 12924
- 13005
- 13085
- 13165
- 13246
- 13326
- 13407
- 13487
- 13567
- 13648
- 13728
- 13809
- 13889
- 13969
- 14050
- 14130
- 14211
- 14291
- 14371
- 14452
- 14532
- 14613
- 14693
- 14773
- 14854
- 14934
- 15015
- 15095
- 15175
- 15256
- 15336
- 15417
- 15497
- 15577
- 15658
- 15738
- 15819
- 15899
- 15979
- 16060
- 16140
- 16221
- 16301
- 16381
- 16462
- 16542
- 16623
- 16703
- 16783
- 16864
- 16944
- 17025
- 17105
- 17185
- 17266
- 17346
- 17427
- 17507
- 17587
- 17668
- 17748
- 17829
- 17909
- 17989
- 18070
- 18150
- 18231
- 18311
- 18391
- 18472
- 18552
- 18633
- 18713
- 18793
- 18874
- 18954
- 19035
- 19115
- 19195
- 19276
- 19356
- 19437
- 19517
- 19597
- 19678
- 19758
- 19839
- 19919
- 20000
- 20000
- 20882
- 21783
- 22702
- 23641
- 24598
- 25575
- 26570
- 27585
- 28618
- 29671
- 30742
- 31833
- 32943
- 34071
- 35219
- 36386
- 37571
- 38776
- 40000
save_steps_start: 50
seed: 100
train_ratio: 0.85
